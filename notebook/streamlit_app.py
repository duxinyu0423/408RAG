import streamlit as st
# from langchain_openai import ChatOpenAI
import os
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
import sys
sys.path.append("notebook") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain_community.vectorstores import Chroma
from zhipuai_llm import ZhipuaiLLM
from langchain_core.prompts import MessagesPlaceholder
from langchain.retrievers import EnsembleRetriever
import os
import shutil
from buildDB import build_DB
def get_retriever_RAG():
    # å®šä¹‰ Embeddings
    embedding = ZhipuAIEmbeddings()
    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    persist_directory = 'dxyu_DB/vector_db/chroma'
    print('æ£€ç´¢æœ¬åœ°å‘é‡æ•°æ®åº“')
    # åŠ è½½æ•°æ®åº“
    vectordb = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )
    return vectordb.as_retriever()

def get_retriever_RAG_temp():
    print('æ£€ç´¢æœ¬åœ°å’Œä¸´æ—¶å‘é‡æ•°æ®åº“')
    persist_directories = ['dxyu_DB/vector_db/chroma', 'UserTMP/vector_db/chroma']
    embedding = ZhipuAIEmbeddings()
    retrievers = []
    
    for dir_path in persist_directories:
        vectordb = Chroma(
            persist_directory=dir_path,
            embedding_function=embedding
        )
        retrievers.append(vectordb.as_retriever())
    
    # å‡è®¾ç­‰æƒé‡åˆå¹¶ï¼Œå¯è°ƒæ•´ weights å‚æ•°
    ensemble_retriever = EnsembleRetriever(retrievers=retrievers, weights=[1.0]*len(retrievers))
    return ensemble_retriever

def combine_docs_RAG(docs):
    return "\n\n".join(doc.page_content for doc in docs["context"])

def get_qa_history_chain_RAG():
    retriever = get_retriever_RAG()
    ## llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
    llm = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1)
    condense_question_system_template = (
        "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ"
        "å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
    )
    condense_question_prompt = ChatPromptTemplate([
            ("system", condense_question_system_template),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])

    retrieve_docs = RunnableBranch(
        (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever, ),
        condense_question_prompt | llm | StrOutputParser() | retriever,
    )

    system_prompt = (
        "ä½ æ˜¯408åŠ©æ•™å°æœå¤§ç‹ï¼Œå¯ä»¥ä¸ºç”¨æˆ·è§£å†³408ç›¸å…³çš„é—®é¢˜"
        # "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ "
        "è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ "
        "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ "
        "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
        # "å¦‚æœç”¨æˆ·é—®ä½ ä½ æ˜¯è°ï¼Œä½ å°±è¯´ä½ æ˜¯æœæ¬£ç‘€"
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    qa_chain = (
        RunnablePassthrough().assign(context=combine_docs_RAG)
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    qa_history_chain = RunnablePassthrough().assign(
        context = retrieve_docs, 
        ).assign(answer=qa_chain)
    return qa_history_chain


# def get_retriever_RAG_temp():
#     embedding = ZhipuAIEmbeddings()
    
#     # åŠ è½½å¤šä¸ªå‘é‡åº“
#     vectordb1 = Chroma(
#         persist_directory='dxyu_DB/vector_db/chroma',  # ç¬¬ä¸€ä¸ªåº“è·¯å¾„
#         embedding_function=embedding
#     )
#     vectordb2 = Chroma(
#         persist_directory='UserTMP/vector_db/chroma',  # ç¬¬äºŒä¸ªåº“è·¯å¾„
#         embedding_function=embedding
#     )
    
#     # åˆ›å»ºæ£€ç´¢å™¨å¹¶è®¾ç½®è¿”å›æ•°é‡
#     retriever1 = vectordb1.as_retriever(search_kwargs={"k": 3})
#     retriever2 = vectordb2.as_retriever(search_kwargs={"k": 3})
    
#     # åˆå¹¶æ£€ç´¢å™¨ï¼ˆå‡è®¾æƒé‡ç›¸åŒï¼‰
#     ensemble_retriever = EnsembleRetriever(
#         retrievers=[retriever1, retriever2],
#         weights=[0.5, 0.5]  # è°ƒæ•´æƒé‡
#     )
#     return ensemble_retriever

def get_qa_history_chain_RAG_temp():
    retriever = get_retriever_RAG_temp()
    ## llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
    llm = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1)
    condense_question_system_template = (
        "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ"
        "å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
    )
    condense_question_prompt = ChatPromptTemplate([
            ("system", condense_question_system_template),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])

    retrieve_docs = RunnableBranch(
        (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever, ),
        condense_question_prompt | llm | StrOutputParser() | retriever,
    )

    system_prompt = (
        "ä½ æ˜¯408åŠ©æ•™å°æœå¤§ç‹ï¼Œå¯ä»¥ä¸ºç”¨æˆ·è§£å†³408ç›¸å…³çš„é—®é¢˜"
        # "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ "
        "è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ "
        "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ "
        "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
        # "å¦‚æœç”¨æˆ·é—®ä½ ä½ æ˜¯è°ï¼Œä½ å°±è¯´ä½ æ˜¯æœæ¬£ç‘€"
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    qa_chain = (
        RunnablePassthrough().assign(context=combine_docs_RAG)
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    qa_history_chain = RunnablePassthrough().assign(
        context = retrieve_docs, 
        ).assign(answer=qa_chain)
    return qa_history_chain
            
def gen_response_RAG(chain, input, chat_history):
    response = chain.stream({
        "input": input,
        "chat_history": chat_history
    })
    for res in response:
        if "answer" in res.keys():
            yield res["answer"]
            
def gen_response_LLM(chain, input, chat_history):
    response = chain.stream({
        "input": input,
        "chat_history": chat_history
    })
    for res in response:
        yield res
            

def get_qa_history_chain_LLM():
    # åˆå§‹åŒ–çº¯LLMï¼ˆæ­¤å¤„ä¿ç•™åŸå‚æ•°é…ç½®ï¼‰
    llm = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1)
    
    # ç²¾ç®€åçš„ç³»ç»Ÿæç¤ºï¼ˆç§»é™¤æ£€ç´¢ç›¸å…³æŒ‡ä»¤ï¼‰
    system_prompt = (
        "è¯·ç”¨ç®€æ´æ˜“æ‡‚çš„è¯­è¨€å›ç­”ç”¨æˆ·æé—®ã€‚"
        "è‹¥ä¸çŸ¥é“ç­”æ¡ˆè¯·è¯´ä¸çŸ¥é“ã€‚"
        "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
    )
    
    # æ„å»ºçº¯å¯¹è¯æç¤ºæ¨¡æ¿
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),  # ä¿ç•™å¯¹è¯å†å²å ä½
        ("human", "{input}"),
    ])

    # ç®€åŒ–å¤„ç†é“¾ï¼ˆç§»é™¤æ‰€æœ‰æ£€ç´¢ç›¸å…³æ“ä½œï¼‰
    qa_chain = (
        RunnablePassthrough()  # ç›´æ¥ä¼ é€’è¾“å…¥
        | qa_prompt 
        | llm
        | StrOutputParser()
    )

    # æœ€ç»ˆå¯¹è¯é“¾ï¼ˆä¿ç•™å¤šè½®å¯¹è¯èƒ½åŠ›ï¼‰
    return qa_chain



            
            
from tempfile import TemporaryDirectory
import os            


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    st.markdown('### ğŸ¦œğŸ”— å°æœå¤§ç‹æ•™ä½ 408')

    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    st.toast('ğŸ‘‹(â‰§âˆ‡â‰¦)ï¾‰ æˆ‘æ˜¯å°æœå¤§ç‹ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜å°½ç®¡é—®æˆ‘å§ï¼âœ¨ å°æç¤ºï¼šå¯ä»¥ä¸Šä¼ ä½ è‡ªå·±çš„æ–‡ä»¶å“¦~ğŸ’¡ è¯•è¯•é—®æˆ‘ï¼š"TCPå’ŒUDPæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"')
        
        
    if "use_rag" not in st.session_state:  # æ–°å¢ï¼šæ§åˆ¶RAGå¼€å…³
        st.session_state.use_rag = True
        
        # å­˜å‚¨æ£€ç´¢é—®ç­”é“¾
    if "qa_history_chain" not in st.session_state and st.session_state.use_rag:
        st.session_state.qa_history_chain = get_qa_history_chain_RAG()
    elif "qa_history_chain" not in st.session_state and not st.session_state.use_rag:
        st.session_state.qa_history_chain = get_qa_history_chain_LLM()

    # æ·»åŠ ä¸‰ä¸ªåŠŸèƒ½æŒ‰é’®ï¼ˆæ¨ªå‘æ’åˆ—ï¼‰
    col1, col2, col3 = st.columns(3)
    
    

    with col1:
        # æ–‡ä»¶ä¸Šä¼ æŒ‰é’®
        uploaded_file = st.file_uploader("ğŸ“ æ·»åŠ æ–‡ä»¶åˆ°RAG", 
                                    type=["pdf", "txt", "md"],
                                    accept_multiple_files=False,
                                    key="file_uploader")
        last_folder_path = 'UserTMP/knowledge_db'
        last_name = ''
        if os.path.isdir(last_folder_path):
            # å¦‚æœå­˜åœ¨è¿™ä¸ªè·¯å¾„
            # è·å–è¯¥è·¯å¾„ä¸‹æ–‡ä»¶åå­—
            file_names = os.listdir(last_folder_path)
            if len(file_names) == 1:
                
                last_name = file_names[0]
        if uploaded_file:
            if last_name == uploaded_file.name:
                print("ğŸš« è¯·ä¸è¦ä¸Šä¼ é‡å¤çš„æ–‡ä»¶ï¼")
            else:
                print("ğŸ‰ æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
                # global last_name
                print('æ–‡ä»¶åï¼š', uploaded_file.name)
                # folder_path = '/home/user/lwh/dxyu/llm-universe-test/UserTMP'
                folder_path = 'UserTMP'
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºç›®å½•
                if os.path.isdir(folder_path):
                    # é€’å½’åˆ é™¤ç›®å½•åŠå…¶å†…å®¹
                    shutil.rmtree(folder_path)
                    print(f"æ–‡ä»¶å¤¹ '{folder_path}' å·²åˆ é™¤ã€‚")
                else:
                    print(f"æ–‡ä»¶å¤¹ '{folder_path}' ä¸å­˜åœ¨ã€‚")
                os.mkdir(folder_path)
                target_folder = folder_path + '/knowledge_db'
                os.mkdir(folder_path + '/knowledge_db')
                print(f"æ–‡ä»¶å¤¹ '{folder_path}' åˆ›å»ºæˆåŠŸã€‚")  
                
                save_path = os.path.join(target_folder, uploaded_file.name)          
                with open(save_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                    
                print('å¼€å§‹åˆ›å»ºä¸´æ—¶æ•°æ®åº“')
                build_DB(folder_path)
                print('ä¸´æ—¶æ•°æ®åº“åˆ›å»ºæˆåŠŸ')
                st.session_state.qa_history_chain = get_qa_history_chain_RAG_temp()  # é‡æ–°åŠ è½½é“¾
                st.toast("æ–‡ä»¶å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ï¼", icon="âœ…")
                print('col1')

    

    with col2:
        # çº¯LLMæ¨¡å¼æŒ‰é’®
        if st.button("ğŸ¤– ä»…LLMæ¨¡å¼", 
                    help="ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”",
                    type="primary" if not st.session_state.use_rag else "secondary"):
            st.toast('ğŸ‘‹(â‰§âˆ‡â‰¦)ï¾‰ æˆ‘æ˜¯å°æœå¤§ç‹ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜å°½ç®¡é—®æˆ‘å§ï¼\n âœ¨ å°æç¤ºï¼šå¯ä»¥ä¸Šä¼ ä½ è‡ªå·±çš„æ–‡ä»¶å“¦~\nğŸ’¡ è¯•è¯•é—®æˆ‘ï¼š"TCPå’ŒUDPæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"')
            st.session_state.use_rag = False
            st.session_state.qa_history_chain = get_qa_history_chain_LLM()  # é‡æ–°åŠ è½½é“¾
            st.rerun()
            print('col2')

    with col3:
        # RAGæ¨¡å¼æŒ‰é’®
        if st.button("ğŸ” RAGæ¨¡å¼", 
                    help="ç»“åˆçŸ¥è¯†åº“æ£€ç´¢ç”Ÿæˆå›ç­”",
                    type="primary" if st.session_state.use_rag else "secondary"):
            st.session_state.use_rag = True
            st.session_state.qa_history_chain = get_qa_history_chain_RAG()  # é‡æ–°åŠ è½½é“¾
            st.rerun()
            print('col3')

            
            
    messages = st.container(height=450)
    # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
    for message in st.session_state.messages:
            with messages.chat_message(message[0]):
                st.write(message[1])
    if prompt := st.chat_input("Say something"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append(("human", prompt))
        with messages.chat_message("human"):
            st.write(prompt)
        answer = ""
        if st.session_state.use_rag:
            answer = gen_response_RAG(
                chain=st.session_state.qa_history_chain,
                input=prompt,
                chat_history=st.session_state.messages
            )
        else:
            answer = gen_response_LLM(
                chain=st.session_state.qa_history_chain,
                input=prompt,
                chat_history=st.session_state.messages
            )
        with messages.chat_message("ai"):
            output = st.write_stream(answer)
        st.session_state.messages.append(("ai", output))
    


if __name__ == "__main__":
    main()
    
